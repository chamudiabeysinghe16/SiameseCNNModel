{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8b67aab-f8c0-4cab-af80-92ebe1df360e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: soundfile in /opt/anaconda3/lib/python3.11/site-packages (0.12.1)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/anaconda3/lib/python3.11/site-packages (from soundfile) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.11/site-packages (from cffi>=1.0->soundfile) (2.21)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5ea94996-86b6-4af9-bfef-dcf7e8ffaf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import soundfile as sf  # Import soundfile library\n",
    "\n",
    "def segment_song(file_path, segment_length=40, save_segments=False, output_dir='segments'):\n",
    "    \"\"\"\n",
    "    Segment a song into fixed-length windows.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path: Path to the audio file (e.g., 'path/to/song.mp3').\n",
    "    - segment_length: Length of each segment in seconds (e.g., 40).\n",
    "    - save_segments: Boolean, whether to save segments as separate audio files.\n",
    "    - output_dir: Directory where segmented audio files will be saved.\n",
    "    \n",
    "    Returns:\n",
    "    - segments: A list of audio arrays, each representing a segment.\n",
    "    \"\"\"\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    \n",
    "    # Calculate the number of samples per segment\n",
    "    samples_per_segment = segment_length * sr\n",
    "    \n",
    "    # Number of segments\n",
    "    num_segments = int(np.floor(len(y) / samples_per_segment))\n",
    "    \n",
    "    segments = []\n",
    "    \n",
    "    for i in range(num_segments):\n",
    "        # Calculate start and end sample for the current segment\n",
    "        start_sample = i * samples_per_segment\n",
    "        end_sample = start_sample + samples_per_segment\n",
    "        \n",
    "        # Extract the segment\n",
    "        segment = y[start_sample:end_sample]\n",
    "        segments.append(segment)\n",
    "        \n",
    "        # Optionally save the segment to disk\n",
    "        if save_segments:\n",
    "            if not os.path.isdir(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            segment_filename = os.path.join(output_dir, f'segment_{i}.wav')\n",
    "            sf.write(segment_filename, segment, sr)  # Use soundfile to save the segment\n",
    "    \n",
    "    return segments\n",
    "\n",
    "# Example usage\n",
    "file_path = '/Users/chamudi/Desktop/songs/train_data/1.mp3'\n",
    "segments = segment_song(file_path, segment_length=40, save_segments=True, output_dir='output_segments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "39ebeaa7-511c-44ba-a5a1-136da9340ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def extract_features(segments, sr=22050, n_fft=2048, hop_length=512):\n",
    "    \"\"\"\n",
    "    Extract STFT-based features from audio segments.\n",
    "    \n",
    "    Parameters:\n",
    "    - segments: List of audio segments.\n",
    "    - sr: Sampling rate of the audio segments.\n",
    "    - n_fft: Number of FFT components.\n",
    "    - hop_length: Number of samples between successive frames.\n",
    "    \n",
    "    Returns:\n",
    "    - fingerprints: A list of magnitude spectrograms (audio fingerprints) for each segment.\n",
    "    \"\"\"\n",
    "    fingerprints = []\n",
    "    for segment in segments:\n",
    "        # Compute the STFT\n",
    "        stft = librosa.stft(segment, n_fft=n_fft, hop_length=hop_length)\n",
    "        \n",
    "        # Compute the magnitude spectrogram from the STFT\n",
    "        spectrogram = np.abs(stft)\n",
    "        \n",
    "        # Convert to decibel units for a more dynamic range\n",
    "        db_spectrogram = librosa.amplitude_to_db(spectrogram, ref=np.max)\n",
    "        \n",
    "        fingerprints.append(db_spectrogram)\n",
    "    \n",
    "    return fingerprints\n",
    "\n",
    "# Example usage assuming 'segments' is a list of audio segments from the previous step\n",
    "fingerprints = extract_features(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "82479943-e391-48d7-9086-fdc9d1beaf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy data initialization for demonstration purposes\n",
    "fingerprints = np.random.rand(100, 128, 44, 1)  # Example shape, adjust as necessary\n",
    "song_ids = np.random.randint(0, 10, size=100)   # Assuming 10 unique songs for 100 fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "541693c8-2c38-4960-9d06-d9dbbf04aa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pairs(fingerprints, song_ids):\n",
    "    positive_pairs = []\n",
    "    negative_pairs = []\n",
    "    labels = []\n",
    "\n",
    "    # Generate positive pairs\n",
    "    for i in range(len(fingerprints)):\n",
    "        for j in range(i+1, len(fingerprints)):\n",
    "            if song_ids[i] == song_ids[j]:\n",
    "                positive_pairs.append([fingerprints[i], fingerprints[j]])\n",
    "                labels.append(1)\n",
    "    \n",
    "    # Generate negative pairs (simplified approach)\n",
    "    for i in range(len(positive_pairs)):  # Generating as many negative pairs as positive\n",
    "        while True:\n",
    "            idx1, idx2 = np.random.randint(0, len(fingerprints), size=2)\n",
    "            if song_ids[idx1] != song_ids[idx2]:\n",
    "                negative_pairs.append([fingerprints[idx1], fingerprints[idx2]])\n",
    "                labels.append(0)\n",
    "                break\n",
    "    \n",
    "    # Combine positive and negative pairs\n",
    "    pairs = positive_pairs + negative_pairs\n",
    "    return np.array(pairs), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "67382f07-d3e5-48b9-84f3-647aec52e1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs, labels = generate_pairs(fingerprints, song_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "853fe156-ec65-4638-af6e-396d4259d1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of splitting (adjust indices according to your dataset size and needs)\n",
    "split_point = int(len(pairs) * 0.8)\n",
    "pairs_train, labels_train = pairs[:split_point], labels[:split_point]\n",
    "pairs_val, labels_val = pairs[split_point:], labels[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7fd89ab1-d65e-4d72-b69e-9908546a505c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Dummy initialization of fingerprints and their corresponding song IDs\n",
    "# Replace this with actual loading or generation of your dataset\n",
    "fingerprints = np.random.rand(100, 128, 44, 1)  # 100 random fingerprints\n",
    "song_ids = np.random.randint(0, 10, 100)       # 100 random song IDs ranging from 0 to 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "02236b51-a06d-4aa3-9415-03e88a85cc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pairs(fingerprints, song_ids):\n",
    "    positive_pairs = []\n",
    "    negative_pairs = []\n",
    "    labels = []\n",
    "\n",
    "    # Example logic to generate positive and negative pairs\n",
    "    for i in range(len(fingerprints)):\n",
    "        for j in range(i + 1, len(fingerprints)):\n",
    "            if song_ids[i] == song_ids[j]:\n",
    "                positive_pairs.append([fingerprints[i], fingerprints[j]])\n",
    "                labels.append(1)  # Similar\n",
    "            else:\n",
    "                if len(negative_pairs) < len(positive_pairs):  # To balance the dataset\n",
    "                    negative_pairs.append([fingerprints[i], fingerprints[j]])\n",
    "                    labels.append(0)  # Dissimilar\n",
    "\n",
    "    pairs = np.array(positive_pairs + negative_pairs)\n",
    "    labels = np.array(labels)\n",
    "    return pairs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5f202a96-eb68-47e6-bcfc-ea507410172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs, labels = generate_pairs(fingerprints, song_ids)  # Use the actual function to generate pairs\n",
    "\n",
    "# Shuffle and split the dataset into training and validation sets\n",
    "indices = np.arange(len(pairs))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "split_point = int(len(pairs) * 0.8)  # 80% for training, 20% for validation\n",
    "train_indices = indices[:split_point]\n",
    "val_indices = indices[split_point:]\n",
    "\n",
    "pairs_train, labels_train = pairs[train_indices], labels[train_indices]\n",
    "pairs_val, labels_val = pairs[val_indices], labels[val_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "406c471d-633e-4b9c-946b-6cc634961643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_31\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_31\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_22      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">44</span>,   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_23      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">44</span>,   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ functional_29       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,498,304</span> │ input_layer_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │ input_layer_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ functional_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │                   │            │ functional_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> │ lambda_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_22      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m44\u001b[0m,   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m1\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_23      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m44\u001b[0m,   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m1\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ functional_29       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │  \u001b[38;5;34m4,498,304\u001b[0m │ input_layer_22[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │ input_layer_23[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_7 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ functional_29[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │                   │            │ functional_29[\u001b[38;5;34m1\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m2\u001b[0m │ lambda_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,498,306</span> (17.16 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,498,306\u001b[0m (17.16 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,498,306</span> (17.16 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,498,306\u001b[0m (17.16 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Lambda\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def create_base_network(input_shape):\n",
    "    \"\"\"\n",
    "    Base network to be shared (eq. to feature extraction).\n",
    "    \"\"\"\n",
    "    input = Input(shape=input_shape)\n",
    "    x = Conv2D(64, (3, 3), activation='relu')(input)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    return Model(input, x)\n",
    "\n",
    "def euclidean_distance(vects):\n",
    "    \"\"\"\n",
    "    Compute Euclidean Distance between two vectors.\n",
    "    \"\"\"\n",
    "    x, y = vects\n",
    "    sum_square = tf.math.reduce_sum(tf.math.square(x - y), axis=1, keepdims=True)\n",
    "    return tf.math.sqrt(tf.maximum(sum_square, tf.keras.backend.epsilon()))\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    \"\"\"\n",
    "    Shape of the output of the Euclidean distance layer.\n",
    "    \"\"\"\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "# Define the tensors for the two input images\n",
    "input_shape = (128, 44, 1)  # Example input shape, adjust based on your fingerprint shape\n",
    "base_network = create_base_network(input_shape)\n",
    "\n",
    "input_a = Input(shape=input_shape)\n",
    "input_b = Input(shape=input_shape)\n",
    "\n",
    "# Because we re-use the same instance `base_network`,\n",
    "# the weights of the network will be shared across the two branches\n",
    "processed_a = base_network(input_a)\n",
    "processed_b = base_network(input_b)\n",
    "\n",
    "# Use a Lambda layer to compute the absolute difference between the feature vectors\n",
    "distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n",
    "\n",
    "# Add a dense layer with a sigmoid unit to generate the similarity score\n",
    "prediction = Dense(1, activation='sigmoid')(distance)\n",
    "\n",
    "# Connect the inputs with the outputs\n",
    "model = Model(inputs=[input_a, input_b], outputs=prediction)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "59ea5da1-10e5-447a-af3a-a48308a3cce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_35\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_35\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_25      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">44</span>,   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_26      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">44</span>,   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ functional_33       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,498,304</span> │ input_layer_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │ input_layer_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ functional_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │                   │            │ functional_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> │ lambda_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_25      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m44\u001b[0m,   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m1\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_26      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m44\u001b[0m,   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m1\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ functional_33       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │  \u001b[38;5;34m4,498,304\u001b[0m │ input_layer_25[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │ input_layer_26[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_8 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ functional_33[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │                   │            │ functional_33[\u001b[38;5;34m1\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m2\u001b[0m │ lambda_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,498,306</span> (17.16 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,498,306\u001b[0m (17.16 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,498,306</span> (17.16 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,498,306\u001b[0m (17.16 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Lambda\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def create_base_network(input_shape):\n",
    "    \"\"\"\n",
    "    Base network to be shared (eq. to feature extraction).\n",
    "    \"\"\"\n",
    "    input = Input(shape=input_shape)\n",
    "    x = Conv2D(64, (3, 3), activation='relu')(input)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    return Model(input, x)\n",
    "\n",
    "def euclidean_distance(vects):\n",
    "    \"\"\"\n",
    "    Compute Euclidean Distance between two vectors.\n",
    "    \"\"\"\n",
    "    x, y = vects\n",
    "    sum_square = tf.reduce_sum(tf.square(x - y), axis=1, keepdims=True)\n",
    "    return tf.sqrt(tf.maximum(sum_square, tf.keras.backend.epsilon()))\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    \"\"\"\n",
    "    Shape of the output of the Euclidean distance layer.\n",
    "    \"\"\"\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "# Define the tensors for the two input images\n",
    "input_shape = (128, 44, 1)  # Example input shape, adjust based on your fingerprint shape\n",
    "base_network = create_base_network(input_shape)\n",
    "\n",
    "input_a = Input(shape=input_shape)\n",
    "input_b = Input(shape=input_shape)\n",
    "\n",
    "# Because we re-use the same instance `base_network`,\n",
    "# the weights of the network will be shared across the two branches\n",
    "processed_a = base_network(input_a)\n",
    "processed_b = base_network(input_b)\n",
    "\n",
    "# Use a Lambda layer to compute the absolute difference between the feature vectors\n",
    "distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n",
    "\n",
    "# Add a dense layer with a sigmoid unit to generate the similarity score\n",
    "prediction = Dense(1, activation='sigmoid')(distance)\n",
    "\n",
    "# Connect the inputs with the outputs\n",
    "model = Model(inputs=[input_a, input_b], outputs=prediction)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "26c152ab-5efa-4e62-aeb5-b8427087723b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "af700b83-a13a-466c-ae1c-d25e3efb398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('/Users/chamudi/Desktop/my_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "733ec676-d3ca-44a7-8b62-6cf6a22b9e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = 'User/chamudi/Desktop/weights.weights.h5'  # Adjusted extension\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "1c2c16f4-e5fd-4c94-8c7c-394bf709e3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = 'User/chamudi/Desktop/model.keras'  # Updated extension for full model saving\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,  # Saving the entire model\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "9a7bc479-0af0-4bd9-aa1a-29b2872d54ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    patience=5,  # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "1fdf6aec-2b3e-4681-ae10-0c792712e112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 206ms/step - accuracy: 0.4811 - loss: 0.7223 - val_accuracy: 0.5196 - val_loss: 0.7117\n",
      "Epoch 2/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 202ms/step - accuracy: 0.5047 - loss: 0.6879 - val_accuracy: 0.5196 - val_loss: 0.6977\n",
      "Epoch 3/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 201ms/step - accuracy: 0.5170 - loss: 0.6701 - val_accuracy: 0.5196 - val_loss: 0.6918\n",
      "Epoch 4/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 198ms/step - accuracy: 0.4922 - loss: 0.6641 - val_accuracy: 0.5196 - val_loss: 0.6935\n",
      "Epoch 5/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 201ms/step - accuracy: 0.4939 - loss: 0.6527 - val_accuracy: 0.5196 - val_loss: 0.6936\n",
      "Epoch 6/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 199ms/step - accuracy: 0.4934 - loss: 0.6450 - val_accuracy: 0.5245 - val_loss: 0.6917\n",
      "Epoch 7/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 199ms/step - accuracy: 0.4969 - loss: 0.6509 - val_accuracy: 0.5245 - val_loss: 0.6855\n",
      "Epoch 8/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 197ms/step - accuracy: 0.5308 - loss: 0.6417 - val_accuracy: 0.5294 - val_loss: 0.6897\n",
      "Epoch 9/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 198ms/step - accuracy: 0.5554 - loss: 0.6344 - val_accuracy: 0.5441 - val_loss: 0.6873\n",
      "Epoch 10/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 198ms/step - accuracy: 0.5675 - loss: 0.6364 - val_accuracy: 0.5490 - val_loss: 0.6809\n",
      "Epoch 11/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 204ms/step - accuracy: 0.5941 - loss: 0.6320 - val_accuracy: 0.5343 - val_loss: 0.6836\n",
      "Epoch 12/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 199ms/step - accuracy: 0.6270 - loss: 0.6205 - val_accuracy: 0.5539 - val_loss: 0.6871\n",
      "Epoch 13/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 200ms/step - accuracy: 0.6017 - loss: 0.6306 - val_accuracy: 0.5686 - val_loss: 0.6764\n",
      "Epoch 14/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 199ms/step - accuracy: 0.6283 - loss: 0.6374 - val_accuracy: 0.5882 - val_loss: 0.6774\n",
      "Epoch 15/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 198ms/step - accuracy: 0.6262 - loss: 0.6307 - val_accuracy: 0.5735 - val_loss: 0.6808\n",
      "Epoch 16/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 198ms/step - accuracy: 0.6587 - loss: 0.6128 - val_accuracy: 0.5882 - val_loss: 0.6836\n",
      "Epoch 17/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 198ms/step - accuracy: 0.6587 - loss: 0.6211 - val_accuracy: 0.5833 - val_loss: 0.6789\n",
      "Epoch 18/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 203ms/step - accuracy: 0.6828 - loss: 0.6170 - val_accuracy: 0.6176 - val_loss: 0.6747\n",
      "Epoch 19/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 199ms/step - accuracy: 0.6645 - loss: 0.6187 - val_accuracy: 0.5833 - val_loss: 0.6798\n",
      "Epoch 20/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 198ms/step - accuracy: 0.6847 - loss: 0.6059 - val_accuracy: 0.5882 - val_loss: 0.6782\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    [pairs_train[:, 0], pairs_train[:, 1]],  # Assuming pairs_train is structured to allow this indexing\n",
    "    labels_train,\n",
    "    validation_data=([pairs_val[:, 0], pairs_val[:, 1]], labels_val),\n",
    "    epochs=20,  # Adjust based on your needs\n",
    "    batch_size=64,  # Adjust based on your needs\n",
    "    callbacks=[model_checkpoint_callback, early_stopping_callback],\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "833874f3-c530-4f39-9ea7-d2bb2d0647f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.4890 - loss: 0.6929\n",
      "Test Loss: 0.6920572519302368\n",
      "Test Accuracy: 0.4699999988079071\n"
     ]
    }
   ],
   "source": [
    "# Generate dummy test data (assuming your inputs are images of shape 128x44x1)\n",
    "import numpy as np\n",
    "\n",
    "num_test_samples = 100\n",
    "pairs_test = np.random.rand(num_test_samples, 2, 128, 44, 1)  # 100 pairs of test fingerprints\n",
    "labels_test = np.random.randint(0, 2, num_test_samples)       # 100 random binary labels\n",
    "\n",
    "# Convert the dummy test data into the correct format\n",
    "left_input_test = np.array([pair[0] for pair in pairs_test])\n",
    "right_input_test = np.array([pair[1] for pair in pairs_test])\n",
    "\n",
    "# Evaluate the model on the dummy test data\n",
    "test_loss, test_accuracy = model.evaluate([left_input_test, right_input_test], labels_test, verbose=1)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "7be075b0-498c-46d7-a924-c9b2cca8b858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.4890 - loss: 0.6929\n",
      "Test Loss: 0.6920572519302368\n",
      "Test Accuracy: 0.4699999988079071\n"
     ]
    }
   ],
   "source": [
    "# Assuming pairs_test and labels_test are your test dataset prepared similarly to training data\n",
    "test_loss, test_accuracy = model.evaluate([pairs_test[:, 0], pairs_test[:, 1]], labels_test, verbose=1)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "3ed35b17-c402-4008-9803-9b4b0ba1b5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n"
     ]
    }
   ],
   "source": [
    "# Assuming left_input_test and right_input_test are your test inputs\n",
    "predictions = model.predict([left_input_test, right_input_test])\n",
    "\n",
    "# Binarize predictions based on a 0.5 threshold\n",
    "binary_predictions = (predictions > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "2ba862d8-fd0d-42b3-b2b0-5e0a8bdf47f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.4792\n",
      "Recall: 0.9388\n",
      "F1 Score: 0.6345\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming labels_test are your true binary labels\n",
    "precision = precision_score(labels_test, binary_predictions)\n",
    "recall = recall_score(labels_test, binary_predictions)\n",
    "f1 = f1_score(labels_test, binary_predictions)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "5b8c3418-bc5a-4524-adf4-31744c0ed751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def format_features(fingerprints):\n",
    "    # Check if fingerprints is a list and has at least one array\n",
    "    if isinstance(fingerprints, list) and all(isinstance(fp, np.ndarray) for fp in fingerprints) and len(fingerprints) > 0:\n",
    "        fingerprints = np.concatenate(fingerprints)\n",
    "    elif not fingerprints:  # fingerprints is empty\n",
    "        # Handle empty fingerprints list: return an empty array or raise an error\n",
    "        return np.array([])  # or raise ValueError(\"No data to format\")\n",
    "    \n",
    "    # Assuming fingerprints now contains data, proceed with normalization and reshaping\n",
    "    normalized_fingerprints = (fingerprints - fingerprints.mean()) / fingerprints.std()\n",
    "    reshaped_fingerprints = normalized_fingerprints.reshape(-1, 1)  # Example reshaping\n",
    "    return reshaped_fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "82f83388-64b5-4645-8f5b-64b1372c0211",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "\n",
    "# Load the input song as a NumPy array\n",
    "input_song, sr = librosa.load('/Users/chamudi/Desktop/songs/unknown_data/10.mp3', sr=None, mono=False)  # Load as stereo\n",
    "\n",
    "# Ensure the input song is a NumPy array\n",
    "if not isinstance(input_song, np.ndarray):\n",
    "    input_song = np.array(input_song)\n",
    "\n",
    "# Now, you can pass the input song to the preprocessing function\n",
    "segments, fingerprints = preprocess_input_song(input_song)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "24c3372b-f281-46ad-8abd-4cf2f9dc4649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preprocess_input_song(input_song):\n",
    "    # Split the input song into segments\n",
    "    segment_length = 40  # Assuming each segment is 40 seconds long\n",
    "    num_segments = len(input_song) // segment_length\n",
    "    segments = []\n",
    "    for i in range(num_segments):\n",
    "        start_idx = i * segment_length\n",
    "        end_idx = (i + 1) * segment_length\n",
    "        segment = input_song[start_idx:end_idx]\n",
    "        segments.append(segment)\n",
    "    segments = np.array(segments)\n",
    "\n",
    "    # Extract features from each segment (e.g., audio fingerprints using STFT)\n",
    "    fingerprints = extract_features(segments)\n",
    "\n",
    "    # Format the data appropriately (e.g., reshape features, normalize)\n",
    "    fingerprints = format_features(fingerprints)\n",
    "\n",
    "    return segments, fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "70aaa16d-074f-44be-924b-8b34e6e1c3df",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[214], line 19\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#model = tf.keras.models.load_model(\u001b[39;00m\n\u001b[1;32m     11\u001b[0m    \u001b[38;5;66;03m# '/Users/chamudi/Desktop/model_version_7.h5',\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m#custom_objects={'euclidean_distance': euclidean_distance}\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Step 3: Predict the Similarity\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Assuming your model takes pairs of fingerprints as input\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict([fingerprints[:, \u001b[38;5;241m0\u001b[39m], fingerprints[:, \u001b[38;5;241m1\u001b[39m]])\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Step 4: Assess Accuracy (if ground truth labels are available)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Assuming you have ground truth labels for similarity between segments\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Calculate accuracy, precision, recall, and F1 score\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Step 5: Output Predictions and Accuracy\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "# Assuming you have already loaded your trained model and preprocessed the input song\n",
    "\n",
    "# Step 1: Preprocess the Input Song\n",
    "# This could involve splitting the song into segments, extracting features, and formatting the data\n",
    "segments, fingerprints = preprocess_input_song(input_song)\n",
    "\n",
    "# Step 2: Load the Trained Model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "#model = tf.keras.models.load_model(\n",
    "   # '/Users/chamudi/Desktop/model_version_7.h5',\n",
    "    #custom_objects={'euclidean_distance': euclidean_distance}\n",
    "#)\n",
    "\n",
    "#model = load_model('/Users/chamudi/Desktop/model_version_7.h5')\n",
    "\n",
    "# Step 3: Predict the Similarity\n",
    "# Assuming your model takes pairs of fingerprints as input\n",
    "predictions = model.predict([fingerprints[:, 0], fingerprints[:, 1]])\n",
    "\n",
    "# Step 4: Assess Accuracy (if ground truth labels are available)\n",
    "# Assuming you have ground truth labels for similarity between segments\n",
    "# Calculate accuracy, precision, recall, and F1 score\n",
    "\n",
    "# Step 5: Output Predictions and Accuracy\n",
    "print(\"Predictions:\")\n",
    "print(predictions)\n",
    "print(\"Accuracy:\")\n",
    "print(accuracy)\n",
    "print(\"Precision:\")\n",
    "print(precision)\n",
    "print(\"Recall:\")\n",
    "print(recall)\n",
    "print(\"F1 Score:\")\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe18550f-fbe5-4147-a6b0-5c042dca4ae9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
